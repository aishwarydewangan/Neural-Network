{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Apparel/apparel-trainval.csv')\n",
    "\n",
    "tempss = data['label']\n",
    "data = data.drop(labels = 'label', axis = 1)\n",
    "\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "\n",
    "data = (data-mean)/std\n",
    "data.insert(0, 'label', tempss, allow_duplicates = False)\n",
    "\n",
    "train = data.sample(frac=0.9, random_state=200)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "train_act = np.array(train['label'])\n",
    "train_act = train_act.reshape(train['label'].count(), 1)\n",
    "\n",
    "test_act = np.array(test['label'])\n",
    "test_act = test_act.reshape(test['label'].count(), 1)\n",
    "\n",
    "train = train.drop(labels = 'label', axis = 1)\n",
    "test = test.drop(labels = 'label', axis = 1)\n",
    "\n",
    "train = np.array(train)\n",
    "train = train.T\n",
    "\n",
    "test = np.array(test)\n",
    "test = test.T\n",
    "\n",
    "\n",
    "one_hot = list()\n",
    "\n",
    "for i in range(train_act.shape[0]) :\n",
    "    temp = [0 for j in range(10)]\n",
    "    temp[train_act[i][0]] = 1\n",
    "\n",
    "    one_hot.append(temp)\n",
    "\n",
    "y_train_encoded = np.array(one_hot)\n",
    "y_train_encoded = y_train_encoded.T\n",
    "\n",
    "\n",
    "batched_train_input = []\n",
    "batched_train_output = []\n",
    "\n",
    "def batch(train, batchsize, no_of_batches) :\n",
    "    global batched_train_input\n",
    "    global batched_train_output\n",
    "\n",
    "    temp = 0\n",
    "    for i in range(no_of_batches) :\n",
    "        batched_train_input.append(train[:,temp*batchsize:temp*batchsize + batchsize])\n",
    "        batched_train_output.append(y_train_encoded[:,temp*batchsize:temp*batchsize + batchsize])\n",
    "        temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "no_of_batches = math.ceil(train.shape[1]/batchsize)\n",
    "batch(train, batchsize, no_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {}\n",
    "B = {}\n",
    "Z = {}\n",
    "A = {}\n",
    "\n",
    "dW = {}\n",
    "dB = {}\n",
    "dZ = {}\n",
    "dA = {}\n",
    "\n",
    "activation = [\"input\", \"relu\", \"relu\", \"softmax\"]\n",
    "\n",
    "layers = [784, 60, 60, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x) :\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x) :\n",
    "    t = np.exp(x)\n",
    "    return t/ np.sum(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return (sigmoid(x) * (1-sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x):\n",
    "    return (1 - np.pow(tanh(x), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    layers = [784, 64, 64, 10]\n",
    "    \n",
    "    L = len(layers)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Matrix = np.random.randn(layers[l], layers[l-1]) * np.sqrt(2/(layers[l]+layers[l-1]))\n",
    "        Matrix = np.array(Matrix)\n",
    "        W[l] = Matrix\n",
    "        B[l] = [[random.uniform(0.01,0.001) for j in range(layers[l])]]\n",
    "        B[l] = np.array(B[l])\n",
    "        B[l] = B[l].T\n",
    "        np.reshape(B[l], layers[l], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X): #activation is list with index 0 containing gibrish\n",
    "    L = len(activation)\n",
    "    \n",
    "    A[0] = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        if l == 1:\n",
    "            Z[l] = np.dot(W[l], X) + B[l]\n",
    "        else:\n",
    "            Z[l] = np.dot(W[l], A[l-1]) + B[l]\n",
    "\n",
    "        if activation[l] == \"sigmoid\":\n",
    "            A[l] = sigmoid(Z[l])\n",
    "        \n",
    "        if activation[l] == \"relu\":\n",
    "            A[l] = relu(Z[l])\n",
    "        \n",
    "        if activation[l] == \"softmax\":\n",
    "            A[l] = softmax(Z[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(y, m):\n",
    "    L = len(activation)-1\n",
    "    \n",
    "    lr = 0.01\n",
    "    \n",
    "    # Calculate gradients\n",
    "    for l in range(L, 0, -1):\n",
    "        if l == L:\n",
    "            dZ[l] = A[l] - y\n",
    "        else:\n",
    "            if activation[l] == \"sigmoid\":\n",
    "                dZ[l] = np.multiply(dA[l], sigmoid_derivative(Z[l]))\n",
    "            \n",
    "            if activation[l] == \"relu\":\n",
    "                dZ[l] = np.multiply(dA[l], relu_derivative(Z[l]))\n",
    "                \n",
    "        dW[l] = ((1/m) * np.dot(dZ[l], A[l-1].T))\n",
    "    \n",
    "        dB[l] = ((1/m) * np.sum(dZ[l], axis=1, keepdims=True))\n",
    "        \n",
    "        if l != 1:\n",
    "            dA[l-1] = np.dot(W[l].T, dZ[l])\n",
    "            \n",
    "    # Update hyperparameters\n",
    "    for l in range(1, L):\n",
    "        W[l] = W[l] - lr*dW[l]\n",
    "        B[l] = B[l] - lr*dB[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    L = len(activation)\n",
    "    \n",
    "    final_Z = {}\n",
    "    final_A = {}\n",
    "    \n",
    "    final_A[0] = data\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        if l == 1:\n",
    "            final_Z[l] = np.dot(W[l], data) + B[l]\n",
    "        else:\n",
    "            final_Z[l] = np.dot(W[l], final_A[l-1]) + B[l]\n",
    "\n",
    "        if activation[l] == \"sigmoid\":\n",
    "            final_A[l] = sigmoid(final_Z[l])\n",
    "        \n",
    "        if activation[l] == \"relu\":\n",
    "            final_A[l] = relu(final_Z[l])\n",
    "        \n",
    "        if activation[l] == \"softmax\":\n",
    "            final_A[l] = softmax(final_Z[l])\n",
    "            \n",
    "    pred = final_A[L-1].argmax(axis=0)\n",
    "\n",
    "    pred = pred.reshape(data.shape[1],1)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_ip, batch_op, no_of_batches, batchSize, iterations):\n",
    "    init()\n",
    "    for i in range(iterations):\n",
    "        for j in range(no_of_batches):\n",
    "            forward(batch_ip[j])\n",
    "\n",
    "            backpropagation(batch_op[j], batchSize)\n",
    "        print('Epoch: ', i)\n",
    "        \n",
    "        pred = predict(test)\n",
    "\n",
    "        stats(test_act, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(actual, predicted):\n",
    "    print(accuracy_score(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(batched_train_input, batched_train_output, no_of_batches, batchsize, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(test)\n",
    "\n",
    "stats(test_act, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
